# Agriculture Inconsistency Detection Pipeline# Agriculture Inconsistency Detection - Setup & Usage Guide



A comprehensive data pipeline for creating an **Indian Agriculture Inconsistency Detection Dataset** by scraping diverse sources, extracting agriculture-related statements using NLP, and generating intelligent statement pairs for manual annotation.## ğŸ“‹ Project Overview



## ğŸ“‹ Project Overview

A comprehensive pipeline for creating an **Indian Agriculture Inconsistency Detection Dataset**. This project scrapes diverse sources, extracts agriculture-related statements using NLP, and generates intelligent statement pairs for manual annotation.

**Goal**: Create 300+ annotated statement pairs to train inconsistency detection models.

This project creates a dataset for detecting inconsistencies in statements about Indian agriculture. It:

**What This Pipeline Does**:

1. ğŸ” **Multi-Source Scraping**: Google search results (via SerpAPI), web articles, Reddit discussions**Goal**: Create 300+ high-quality annotated statement pairs to train inconsistency detection models.

2. ğŸ§  **NLP Processing**: SpaCy-based statement extraction with opinion detection

3. ğŸ”— **Intelligent Pairing**: Sentence Transformers for semantic similarity with stratified sampling## ğŸ“‹ Project Overview

4. ğŸ“Š **Export for Annotation**: CSV format with metadata for manual labeling

1. **Scrapes** diverse sources (Google, articles, Reddit, social media)

### Task Definition

2. **Extracts** agriculture-related statements using NLP

Classify relationships between agriculture statement pairs:

### What This Does

- **Unrelated**: Statements discuss different topics

- **Consistent**: Both statements can be true, support similar conclusions1. ğŸ” **Multi-Source Scraping**: Google search (via SerpAPI), web articles, Reddit discussions

- **Inconsistent**: Statements contradict each other

  - *Surface contradiction*: Direct logical contradiction2. ğŸ§  **NLP Processing**: SpaCy-based statement extraction with opinion detectionA comprehensive pipeline for creating an **Indian Agriculture Inconsistency Detection Dataset**. This project scrapes diverse sources, extracts agriculture-related statements using NLP, and generates intelligent statement pairs for manual annotation.3. **Generates** smart statement pairs based on semantic similarity

  - *Factual inconsistency*: Conflicting facts/statistics

  - *Value inconsistency*: Conflicting values/policy positions3. ğŸ”— **Intelligent Pairing**: Sentence Transformers for semantic similarity with stratified sampling



---4. ğŸ“Š **Export for Annotation**: CSV format with metadata for manual labeling4. **Exports** pairs for manual annotation



## ğŸš€ Quick Start



### Prerequisites### Task Definition**Goal**: Create 300+ high-quality annotated statement pairs to train inconsistency detection models.



- Python 3.11+

- Virtual environment (recommended)

- Git LFS installed (for data files)Classify relationships between agriculture statement pairs:**Goal**: Create 200+ annotated statement pairs to train an inconsistency detection model.



### 1. Clone Repository



```bash- **Unrelated**: Statements discuss different topics### What This Does

git clone git@github.com:XAheli/AgriIR_Query_Gen.git

cd AgriIR_Query_Gen- **Consistent**: Both statements can be true, support similar conclusions

git lfs pull  # Download data files

```- **Inconsistent**: Statements contradict each other## ğŸ¯ Task Definition



### 2. Install Dependencies  - *Surface contradiction*: Direct logical contradiction



```bash  - *Factual inconsistency*: Conflicting facts/statistics1. ğŸ” **Multi-Source Scraping**: Google search results (SerpAPI), web articles, Reddit discussions

# Create and activate virtual environment

python -m venv .venv  - *Value inconsistency*: Conflicting values/policy positions

source .venv/bin/activate  # On Windows: .venv\Scripts\activate

2. ğŸ§  **NLP Processing**: SpaCy-based statement extraction with opinion detectionClassify relationship between statement pairs as:

# Install packages

pip install -r requirements.txt---



# Download SpaCy model3. ğŸ”— **Intelligent Pairing**: Sentence Transformers for semantic similarity, stratified sampling for diversity- **Unrelated**: Different topics

python -m spacy download en_core_web_sm

```## ğŸš€ Quick Start



### 3. Configure API Keys (Optional but Recommended)4. ğŸ“Š **Export for Annotation**: CSV format with metadata for manual labeling- **Consistent**: Compatible statements



Create `secrets.toml` in the project root:### Prerequisites



```toml- **Inconsistent**: Contradictory statements

[api]

serp_api_key = "your_serpapi_key_here"  # Get from serpapi.com (100 free/month)- Python 3.11+



[reddit]- Virtual environment (recommended)### Task Definition  - *Surface contradiction*: Direct logical contradiction

client_id = "your_reddit_client_id"

client_secret = "your_reddit_client_secret"- Git LFS installed (for data files)

user_agent = "python:agriculture_scraper:v1.0 (by /u/YourUsername)"

```  - *Factual inconsistency*: Conflicting facts/numbers



**Note**: Get Reddit credentials from https://www.reddit.com/prefs/apps### 1. Clone Repository



**Without API keys**: Pipeline will use web scraping fallbacks (slower, less reliable).Classify relationships between agriculture statement pairs:  - *Value inconsistency*: Conflicting values/policies



### 4. Run the Pipeline```bash



```bashgit clone git@github.com:XAheli/AgriIR_Query_Gen.git

source .venv/bin/activate

python main_enhanced.pycd AgriIR_Query_Gen

```

```- **Unrelated**: Statements discuss different topics## ğŸš€ Quick Start

âš ï¸ **Important**: The pipeline may hang at Step 6 (embedding computation) due to GPU/memory limitations on local machines. See "GPU Issue Workaround" section below.



---

### 2. Install Dependencies- **Consistent**: Both statements can be true, support similar conclusions

## ğŸ“‚ Project Structure



```

agriculture_inconsistency_detection/```bash- **Inconsistent**: Statements contradict each other### 1. Install Dependencies

â”œâ”€â”€ main_enhanced.py              # Main pipeline orchestrator

â”œâ”€â”€ config.py                     # Configuration (queries, parameters, loads secrets)# Create and activate virtual environment

â”œâ”€â”€ secrets.toml                  # API keys (LOCAL ONLY - not in repo)

â”œâ”€â”€ requirements.txt              # Python dependenciespython -m venv .venv  - *Surface contradiction*: Direct logical contradiction

â”œâ”€â”€ LICENSE                       # MIT License

â”œâ”€â”€ README.md                     # This filesource .venv/bin/activate  # On Windows: .venv\Scripts\activate

â”‚

â”œâ”€â”€ scraping/                     # Data collection modules  - *Factual inconsistency*: Conflicting facts/statistics```bash

â”‚   â”œâ”€â”€ enhanced_serp_scraper.py  # Google search with SerpAPI

â”‚   â”œâ”€â”€ enhanced_content_scraper.py # Multi-strategy content extraction# Install packages

â”‚   â””â”€â”€ reddit_scraper.py         # Reddit posts & comments

â”‚pip install -r requirements.txt  - *Value inconsistency*: Conflicting values/policy positions# Create virtual environment (recommended)

â”œâ”€â”€ processing/                   # NLP processing modules

â”‚   â”œâ”€â”€ enhanced_statement_extractor.py  # SpaCy + opinion detection

â”‚   â””â”€â”€ enhanced_pair_generator.py       # Semantic similarity pairing

â”‚# Download SpaCy modelpython -m venv venv

â”œâ”€â”€ storage/                      # Database management

â”‚   â””â”€â”€ database.py               # SQLite operationspython -m spacy download en_core_web_sm

â”‚

â”œâ”€â”€ annotation/                   # Export utilities```---source venv/bin/activate  # On Windows: venv\Scripts\activate

â”‚   â””â”€â”€ export_for_annotation.py # CSV/JSON exporters

â”‚

â””â”€â”€ data/                         # Generated data (tracked with Git LFS)

    â”œâ”€â”€ raw/                      # Scraped content### 3. Configure API Keys (Optional but Recommended)

    â”‚   â”œâ”€â”€ search_results.csv

    â”‚   â”œâ”€â”€ documents.json

    â”‚   â””â”€â”€ reddit_content.json

    â”œâ”€â”€ processed/                # Extracted statementsCreate `secrets.toml` with your API keys:## ğŸš€ Quick Start# Install requirements

    â”‚   â””â”€â”€ statements.json

    â”œâ”€â”€ final/                    # Annotated pairs

    â”‚   â””â”€â”€ pairs_for_annotation_*.csv

    â””â”€â”€ agriculture_statements.db # SQLite database```tomlpip install -r requirements.txt

```

[api]

---

serp_api_key = "your_serpapi_key_here"  # Get from serpapi.com (100 free/month)### Prerequisites

## ğŸ“Š Pipeline Steps



The pipeline has **8 steps** divided into two parts:

[reddit]# Download SpaCy model

### Part 1: Data Collection (Steps 1-5)

**Runs on**: Local machine  client_id = "your_reddit_client_id"

**Runtime**: 30-60 minutes

client_secret = "your_reddit_client_secret"- Python 3.11+python -m spacy download en_core_web_sm

1. **SERP Scraping**: Query Google for agriculture-related content using 43 controversy-focused queries

2. **Content Extraction**: Scrape full text from URLs using multi-strategy extractionuser_agent = "python:agriculture_scraper:v1.0 (by /u/YourUsername)"

3. **Reddit Scraping**: Collect posts and comments from 10 agriculture-related subreddits

4. **Statement Extraction**: Use SpaCy to extract sentences, detect opinions, filter for relevance```- Virtual environment (recommended)```

5. **Database Storage**: Save statements to SQLite database



**Expected Output**:

- ~3,400+ documents scraped**Without API keys**: Pipeline will use web scraping fallbacks (slower, less reliable).- Git LFS installed (for data files)

- ~22,000+ statements extracted

- ~4,400+ opinion statements (19.9%)



### Part 2: Pair Generation (Steps 6-8)### 4. Run the Pipeline### 2. Configure (Optional)

**GPU-intensive** - May crash on local machines without sufficient GPU/memory



6. **Embedding Computation**: Generate semantic embeddings using Sentence Transformers

7. **Intelligent Pairing**: ```bash### 1. Clone Repository

   - Compute cosine similarity matrix

   - Quality scoring (bonus for same-source, opinions)source .venv/bin/activate

   - Stratified sampling (50% same-source opinions, 25% same-source mixed, etc.)

   - Diversity filtering (max 100 pairs per URL combo)python main_enhanced.pyEdit `config.py` to:

8. **Export**: Generate CSV with annotation columns

```

**Expected Output**:

- 1,000+ diverse statement pairs ready for annotation```bash- Add more agriculture queries



---âš ï¸ **Note**: The pipeline may hang at Step 6 (embedding computation) due to GPU/memory limitations. See "GPU Issue Workaround" below if this happens.



## ğŸ”§ GPU Issue Workaroundgit clone git@github.com:XAheli/AgriIR_Query_Gen.git- Configure target domains



If your computer hangs/crashes at Step 6 (embedding computation), you have two options:---



### Option A: Stop After Step 5 (Recommended for Low-End Machines)cd AgriIR_Query_Gen- Set API keys (optional, for better results)



```bash## ğŸ“‚ Project Structure

# Run pipeline

python main_enhanced.py```



# Wait for console message: "Step 5 completed: 22,198 statements saved to database"```

# Then press Ctrl+C to stop before Step 6

```agriculture_inconsistency_detection/```python



Your data is saved in:â”œâ”€â”€ main_enhanced.py              # Main pipeline orchestrator

- `data/processed/statements.json` (22,198 statements)

- `data/agriculture_statements.db` (SQLite database)â”œâ”€â”€ config.py                     # Configuration (queries, parameters, loads secrets)### 2. Install Dependencies# For better Google scraping (100 free searches/month)



You can manually create pairs later or use external GPU resources for Steps 6-8.â”œâ”€â”€ secrets.toml                  # API keys (DO NOT COMMIT - not in repo)



### Option B: Use Google Colab (For Full Pipeline)â”œâ”€â”€ requirements.txt              # Python dependenciesSERP_API_KEY = "your_serpapi_key"  # Get from serpapi.com



If you need the pair generation functionality:â”‚



1. Create a Jupyter notebook for Steps 6-8â”œâ”€â”€ scraping/                     # Data collection modules```bash

2. Upload `statements.json` to Google Colab

3. Enable GPU: Runtime â†’ Change runtime type â†’ GPU (T4 or better)â”‚   â”œâ”€â”€ enhanced_serp_scraper.py  # Google search with SerpAPI

4. Run embedding computation with GPU acceleration

5. Download generated pairs CSVâ”‚   â”œâ”€â”€ enhanced_content_scraper.py # Multi-strategy content extraction# Create and activate virtual environment# For Reddit scraping (optional)



---â”‚   â””â”€â”€ reddit_scraper.py         # Reddit posts & comments



## ğŸ”§ Configurationâ”‚python -m venv .venvREDDIT_CLIENT_ID = "your_client_id"



### Key Parameters in `config.py`â”œâ”€â”€ processing/                   # NLP processing modules



```pythonâ”‚   â”œâ”€â”€ enhanced_statement_extractor.py  # SpaCy + opinion detectionsource .venv/bin/activate  # On Windows: .venv\Scripts\activateREDDIT_CLIENT_SECRET = "your_client_secret"

# Queries - 43 controversy-focused queries on Indian agriculture

AGRICULTURE_QUERIES = [...]  # Farm laws, MSP, subsidies, reforms, etc.â”‚   â””â”€â”€ enhanced_pair_generator.py       # Semantic similarity pairing



# Similarity threshold for pairing (0-1)â”‚# Get from: https://www.reddit.com/prefs/apps

SIMILARITY_THRESHOLD = 0.3  # Lower = more pairs but less similar

â”œâ”€â”€ storage/                      # Database management

# Maximum pairs from single URL combination

MAX_PAIRS_PER_SOURCE = 100  # For diversityâ”‚   â””â”€â”€ database.py               # SQLite operations# Install packages```



# Sentence Transformer modelâ”‚

SENTENCE_TRANSFORMER_MODEL = "all-MiniLM-L6-v2"

â”œâ”€â”€ annotation/                   # Export utilitiespip install -r requirements.txt

# Target domains - 20+ sites including government, news, agriculture portals

TARGET_DOMAINS = [...]â”‚   â””â”€â”€ export_for_annotation.py # CSV/JSON exporters

```

â”‚### 3. Run the Pipeline

### Customization

â””â”€â”€ data/                         # Generated data (tracked with Git LFS)

**Edit `config.py` to**:

- Add/modify agriculture queries (focus on controversies)    â”œâ”€â”€ raw/                      # Scraped content# Download SpaCy model

- Change target domains for different source types

- Adjust similarity threshold (lower = more pairs)    â”‚   â”œâ”€â”€ search_results.csv

- Configure scraping parameters (delays, user agent)

    â”‚   â”œâ”€â”€ documents.jsonpython -m spacy download en_core_web_sm```bash

**Edit `main_enhanced.py` to**:

- Change number of queries used (default: all 43)    â”‚   â””â”€â”€ reddit_content.json

- Adjust max URLs per query (default: 50)

- Set target pair count (default: 1000)    â”œâ”€â”€ processed/                # Extracted statements```# Use enhanced pipeline (recommended)

- Enable/disable Reddit scraping (default: enabled)

    â”‚   â””â”€â”€ statements.json

---

    â”œâ”€â”€ final/                    # Annotated pairspython main_enhanced.py

## ğŸ“ Annotation Guide

    â”‚   â””â”€â”€ pairs_for_annotation_*.csv

### CSV Format

    â””â”€â”€ agriculture_statements.db # SQLite database### 3. Configure API Keys (Optional but Recommended)

The exported CSV (`data/final/pairs_for_annotation_*.csv`) contains:

```

| Column | Description |

|--------|-------------|# Or use basic pipeline

| `id` | Unique pair identifier |

| `statement_a` | First statement text |---

| `statement_b` | Second statement text |

| `similarity_score` | Semantic similarity (0-1) |Create `secrets.toml` from the example:python main.py

| `quality_score` | Quality score with bonuses |

| `same_source` | Both from same URL? |## ğŸ“Š Pipeline Steps

| `both_have_opinions` | Both contain opinions? |

| `source_a` / `source_b` | Source URLs |```

| `domain_a` / `domain_b` | Domain names |

| `author_a` / `author_b` | Authors (if available) |The pipeline has **8 steps** divided into two parts:

| `relationship_label` | **[TO FILL]** Unrelated/Consistent/Inconsistent |

| `inconsistency_subtype` | **[TO FILL]** Surface/Factual/Value (if Inconsistent) |```bash

| `notes` | Optional observations |

### Part 1: Data Collection (Steps 1-5)

### Annotation Instructions

cp secrets.toml.example secrets.toml## ğŸ“¦ What Gets Generated

1. **Prioritize**: Start with pairs where `same_source=True` and `both_have_opinions=True`

2. **Label `relationship_label`**:**Runs on: Local machine**

   - `Unrelated`: Completely different topics

   - `Consistent`: Can both be true```

   - `Inconsistent`: Contradictory

3. **If Inconsistent, label `inconsistency_subtype`**:1. **SERP Scraping**: Query Google for agriculture-related content using 43 controversy-focused queries

   - `Surface`: Direct logical contradiction

   - `Factual`: Conflicting facts/numbers2. **Content Extraction**: Scrape full text from URLs using multi-strategy extractionAfter running the pipeline, you'll have:

   - `Value`: Conflicting opinions/values

4. **Add notes**: Any observations to help with model training3. **Reddit Scraping**: Collect posts and comments from 10 agriculture-related subreddits



**Target**: Annotate 300+ pairs for robust dataset.4. **Statement Extraction**: Use SpaCy to extract sentences, detect opinions, filter for relevanceEdit `secrets.toml` and add your API keys:



### Annotation Examples5. **Database Storage**: Save statements to SQLite database



**Inconsistent - Surface:**```

- A: "Farm laws were repealed in 2021"

- B: "Farm laws are still in effect"**Expected Output**:



**Inconsistent - Factual:**- ~3,400+ documents scraped```tomldata/

- A: "MSP increased by 10%"

- B: "MSP decreased by 5%" (same year/crop)- ~22,000+ statements extracted



**Inconsistent - Value:**- ~4,400+ opinion statements (19.9%)[api]â”œâ”€â”€ raw/

- A: "Farm laws benefit small farmers"

- B: "Farm laws harm small farmers"- Runtime: 30-60 minutes



**Consistent:**serp_api_key = "your_serpapi_key_here"  # Get from serpapi.com (100 free/month)â”‚   â”œâ”€â”€ search_results.csv      # Google search results

- A: "Farmers need better MSP"

- B: "Agricultural support prices should increase"### Part 2: Pair Generation (Steps 6-8)



---â”‚   â”œâ”€â”€ documents.json          # Scraped article content



## ğŸ”’ Security & Privacy**GPU-intensive - May crash on local machines**



### API Keys[reddit]â”‚   â””â”€â”€ reddit_content.json     # Reddit posts/comments (if enabled)



- **secrets.toml is NOT in the repository** (protected by .gitignore)6. **Embedding Computation**: Generate semantic embeddings using Sentence Transformers

- `config.py` loads secrets at runtime using `tomli` library

- Never commit `secrets.toml` to version control7. **Intelligent Pairing**: client_id = "your_reddit_client_id"â”œâ”€â”€ processed/

- Get your own API keys:

  - SerpAPI: https://serpapi.com (100 free searches/month)   - Compute cosine similarity matrix

  - Reddit: https://www.reddit.com/prefs/apps

   - Quality scoring (bonus for same-source, opinions)client_secret = "your_reddit_client_secret"â”‚   â””â”€â”€ statements.json         # Extracted statements

### Data Privacy

   - Stratified sampling (50% same-source opinions, 25% same-source mixed, etc.)

- All scraped content is from public sources

- Reddit scraping follows API terms of service   - Diversity filtering (max 100 pairs per URL combo)user_agent = "python:agriculture_scraper:v1.0 (by /u/YourUsername)"â”œâ”€â”€ final/

- Respects `robots.txt` and rate limits

- No personal data collection8. **Export**: Generate CSV with annotation columns



---```â”‚   â””â”€â”€ pairs_for_annotation_[timestamp].csv  # Ready for annotation



## ğŸ“¦ Git LFS**Expected Output**:



Large data files (.csv, .json, .db) are tracked with Git LFS:- 1,000+ diverse statement pairs ready for annotationâ””â”€â”€ agriculture_statements.db   # SQLite database



```bash

# Already configured in .gitattributes

# Files are automatically managed by Git LFS---**Without API keys**: The pipeline will still work using web scraping fallbacks (slower, less reliable).```



# To download data files after cloning

git lfs pull

```## ğŸ”§ GPU Issue Workaround



---



## ğŸ› TroubleshootingIf your computer hangs/crashes at Step 6 (embedding computation), you have two options:### 4. Run the Pipeline## ğŸ“ Manual Annotation



### Import Error: tomli



```bash### Option A: Wait for Step 5 to Complete, Then Stop

pip install tomli==2.0.1

```



### SpaCy Model Not Found```bash#### Option A: Local Machine (Steps 1-5 only)1. **Open** the CSV file in `data/final/`



```bash# Run pipeline

python -m spacy download en_core_web_sm

```python main_enhanced.py2. **Review** each pair and fill columns:



### Warning: secrets.toml not found



This is normal if you haven't created `secrets.toml` yet. The pipeline will work without it using web scraping fallbacks, but:# Wait for console message: "Step 5 completed: 22,198 statements saved to database"```bash   - `relationship_label`: Unrelated/Consistent/Inconsistent

- Google scraping will be slower and less reliable

- Reddit scraping will be disabled# Then press Ctrl+C to stop before Step 6



Create `secrets.toml` with your API keys to enable full functionality.```source .venv/bin/activate   - `inconsistency_subtype`: Surface/Factual/Value (if Inconsistent)



### Pipeline Hangs at Step 6



**Expected behavior** - embedding computation requires significant GPU/memory. Options:Your data is saved in:python main_enhanced.py   - `notes`: Any observations

1. Let Step 5 complete, then stop (Ctrl+C)

2. Use a machine with better GPU/memory- `data/processed/statements.json` (22,198 statements)

3. Modify code to process in smaller batches

- `data/agriculture_statements.db` (SQLite database)```

### No Statements Extracted



**Causes**:

- Scraping failed (check `data/raw/documents.json`)You can manually create pairs later or skip Steps 6-8 if you only need the statements.3. **Prioritize** pairs where `same_source = True` (self-inconsistency)

- Content quality poor

- Filters too strict



**Solutions**:### Option B: Use Google Colab (Not Provided)âš ï¸ **Note**: Steps 6-8 (embedding computation) require GPU resources. Your local machine may hang/crash at Step 6.4. **Target** 200+ annotated pairs for quality dataset

- Verify API keys in `secrets.toml`

- Check internet connection

- Lower `MIN_STATEMENT_LENGTH` in `config.py`

- Add more/better queriesIf you need the pair generation functionality, you would need to:



### SerpAPI Quota Exceeded1. Create a Jupyter notebook for Steps 6-8



- Free tier: 100 searches/month2. Upload `statements.json` to Google Colab#### Option B: Complete Pipeline with Google Colab (Recommended)### Annotation Guidelines

- Wait for quota reset or upgrade plan

- Pipeline automatically falls back to web scraping3. Run embedding computation with GPU (T4 or better)



---4. Download generated pairs CSV



## ğŸ“ˆ Performance Stats (Real Run)



- **Queries Used**: 43 controversy-focused queries---**Steps 1-5 (Local - Data Collection):**#### Unrelated

- **URLs Found**: ~3,400+

- **Documents Scraped**: 3,438 (success rate varies by source)

- **Statements Extracted**: 22,198 total

- **Opinion Statements**: 4,413 (19.9%)## ğŸ”§ Configuration```bash- Statements discuss completely different topics

- **Average Statements/Document**: 6.5

- **Runtime**: ~60 minutes for Steps 1-5



---### Key Parameters in `config.py`source .venv/bin/activate- No logical connection



## ğŸ“ Tips for Better Results



### 1. Target Controversial Topics```pythonpython main_enhanced.py- Example: "MSP increased" vs "Cotton prices fell"



Focus queries on topics with multiple viewpoints:# Queries - 43 controversy-focused queries on Indian agriculture

- Farm laws debate

- MSP policy changesAGRICULTURE_QUERIES = [...]  # Farm laws, MSP, subsidies, reforms, etc.# Wait for "Step 5 completed" message, then stop (Ctrl+C) if it hangs at Step 6

- Subsidy programs

- Agricultural reforms



### 2. Mix Source Types# Similarity threshold for pairing (0-1)```#### Consistent



Combine:SIMILARITY_THRESHOLD = 0.3  # Lower = more pairs but less similar

- Government announcements (official stance)

- News articles (factual reporting)- Both can be true simultaneously

- Opinion pieces (subjective views)

- Social media (public opinions)# Maximum pairs from single URL combination



### 3. Same-Source PairsMAX_PAIRS_PER_SOURCE = 100  # For diversity**Steps 6-8 (Colab - Pair Generation):**- Support similar conclusions



Prioritize pairs from same source/author:

- Better for detecting self-inconsistency

- Shows evolution of positions# Sentence Transformer model1. Upload `colab_pair_generation.ipynb` to Google Colab- Example: "Farmers need support" vs "Agricultural subsidies help farmers"

- More interesting contradictions

SENTENCE_TRANSFORMER_MODEL = "all-MiniLM-L12-v2"

### 4. Opinion Statements

2. Upload `data/processed/statements.json` (generated from Steps 1-5)

Look for statements with:

- Modal verbs: should, must, need to# Target domains - 20+ sites including government, news, agriculture portals

- Stance markers: support, oppose, believe

- Value judgments: better, worse, unfairTARGET_DOMAINS = [...]3. Enable GPU: Runtime â†’ Change runtime type â†’ GPU (T4)#### Inconsistent Types



---```



## ğŸ” Data Quality Checks4. Run all cells



Before annotation, verify:### Customization



**1. Statement Quality**5. Download generated CSV file**Surface Contradiction**

- âœ… Complete sentences

- âœ… Agriculture-relatedEdit `config.py` to:

- âœ… Readable and clear

- âŒ No promotional text- Add/modify agriculture queries (focus on controversies for better inconsistencies)- Direct logical contradiction

- âŒ No navigation text

- Change target domains for different source types

**2. Pair Quality**

- âœ… Semantically similar (overlap in topic)- Adjust similarity threshold (lower = more pairs)**Expected Time**:- Both cannot be true

- âœ… Both are substantial statements

- âœ… Worth comparing for consistency- Configure scraping parameters (delays, user agent)

- âŒ Not identical/near-duplicate

- âŒ Not completely unrelated- Local Steps 1-5: 30-60 minutes (depends on network speed)- Example: "MSP increased by 10%" vs "MSP decreased this year"



---Edit `main_enhanced.py` to:



## ğŸ”— Useful Resources- Change number of queries used (default: all 43)- Colab Steps 6-8: 5-10 minutes with GPU



### APIs & Tools- Adjust max URLs per query (default: 50)

- SerpAPI: https://serpapi.com (Google scraping)

- Reddit API: https://www.reddit.com/prefs/apps- Set target pair count (default: 1000)**Factual Inconsistency**

- SpaCy: https://spacy.io/usage/models

- Enable/disable Reddit scraping (default: enabled)

### Open Source Libraries

- newspaper3k: Article extraction---- Conflicting facts, numbers, or data

- trafilatura: Web content extraction

- PRAW: Reddit API wrapper---

- Sentence Transformers: Semantic embeddings

- Example: "1000 farmers" vs "5000 farmers" (same event)

### Agriculture Sources

- PIB: https://pib.gov.in## ğŸ“ Annotation Guide

- Ministry of Agriculture: https://agricoop.nic.in

- Down to Earth: https://downtoearth.org.in## ğŸ“‚ Project Structure

- The Hindu (Agriculture section)

- Indian Express (Rural section)### CSV Format



---**Value Inconsistency**



## ğŸ¤ ContributingThe exported CSV (`data/final/pairs_for_annotation_*.csv`) contains:



Contributions welcome! Please:```- Conflicting values, opinions, or policy positions

1. Fork the repository

2. Create a feature branch| Column | Description |

3. Make your changes

4. Submit a pull request|--------|-------------|agriculture_inconsistency_detection/- Example: "Farm laws benefit farmers" vs "Farm laws harm farmers"



---| `id` | Unique pair identifier |



## ğŸ“„ License| `statement_a` | First statement text |â”œâ”€â”€ main_enhanced.py              # Main pipeline orchestrator



MIT License - See LICENSE file for details| `statement_b` | Second statement text |



---| `similarity_score` | Semantic similarity (0-1) |â”œâ”€â”€ config.py                     # Configuration (queries, parameters)## ğŸ”§ Customization



## ğŸ™ Acknowledgments| `quality_score` | Quality score with bonuses |



- **SerpAPI** for Google search API| `same_source` | Both from same URL? |â”œâ”€â”€ secrets.toml                  # API keys (DO NOT COMMIT)

- **Reddit API** (PRAW) for social media data

- **SpaCy** for NLP processing| `both_have_opinions` | Both contain opinions? |

- **Sentence Transformers** for semantic embeddings

- **Hugging Face** for transformer models| `source_a` / `source_b` | Source URLs |â”œâ”€â”€ secrets.toml.example          # Template for secrets### Add More Queries



---| `domain_a` / `domain_b` | Domain names |



## ğŸ“§ Contact| `author_a` / `author_b` | Authors (if available) |â”œâ”€â”€ requirements.txt              # Python dependencies



- GitHub Issues: [Report bugs or request features](https://github.com/XAheli/AgriIR_Query_Gen/issues)| `relationship_label` | **[TO FILL]** Unrelated/Consistent/Inconsistent |

- GitHub: [@XAheli](https://github.com/XAheli)

| `inconsistency_subtype` | **[TO FILL]** Surface/Factual/Value (if Inconsistent) |â”œâ”€â”€ colab_pair_generation.ipynb   # Google Colab notebook for Steps 6-8Edit `config.py`:

---

| `notes` | Optional observations |

## ğŸ—ºï¸ Roadmap

â”‚

- [ ] Add support for Twitter/X scraping

- [ ] Implement automatic annotation suggestions### Annotation Instructions

- [ ] Add data augmentation techniques

- [ ] Create fine-tuned inconsistency detection modelâ”œâ”€â”€ scraping/                     # Data collection modules```python

- [ ] Deploy as web service API

- [ ] Add multilingual support (Hindi, other Indian languages)1. **Prioritize**: Start with pairs where `same_source=True` and `both_have_opinions=True`



---2. **Label `relationship_label`**:â”‚   â”œâ”€â”€ enhanced_serp_scraper.py  # Google search with SerpAPIAGRICULTURE_QUERIES = [



**Star â­ this repo if you find it useful!**   - `Unrelated`: Completely different topics


   - `Consistent`: Can both be trueâ”‚   â”œâ”€â”€ enhanced_content_scraper.py # Multi-strategy content extraction    "your custom query 1",

   - `Inconsistent`: Contradictory

3. **If Inconsistent, label `inconsistency_subtype`**:â”‚   â””â”€â”€ reddit_scraper.py         # Reddit posts & comments    "your custom query 2",

   - `Surface`: Direct logical contradiction

   - `Factual`: Conflicting facts/numbersâ”‚    # Add more...

   - `Value`: Conflicting opinions/values

4. **Add notes**: Any observations to help with model trainingâ”œâ”€â”€ processing/                   # NLP processing modules]



**Target**: Annotate 300+ pairs for robust dataset.â”‚   â”œâ”€â”€ enhanced_statement_extractor.py  # SpaCy + opinion detection```



### Annotation Examplesâ”‚   â””â”€â”€ enhanced_pair_generator.py       # Semantic similarity pairing



**Inconsistent - Surface:**â”‚### Adjust Similarity Threshold

- A: "Farm laws were repealed in 2021"

- B: "Farm laws are still in effect"â”œâ”€â”€ storage/                      # Database management



**Inconsistent - Factual:**â”‚   â””â”€â”€ database.py               # SQLite operationsLower threshold = more pairs (but less similar):

- A: "MSP increased by 10%"

- B: "MSP decreased by 5%" (same year/crop)â”‚



**Inconsistent - Value:**â”œâ”€â”€ annotation/                   # Export utilities```python

- A: "Farm laws benefit small farmers"

- B: "Farm laws harm small farmers"â”‚   â””â”€â”€ export_for_annotation.py # CSV/JSON exportersSIMILARITY_THRESHOLD = 0.2  # Default: 0.3



**Consistent:**â”‚```

- A: "Farmers need better MSP"

- B: "Agricultural support prices should increase"â””â”€â”€ data/                         # Generated data (tracked with Git LFS)



---    â”œâ”€â”€ raw/                      # Scraped content### Change Target Pair Count



## ğŸ”’ Security & Privacy    â”‚   â”œâ”€â”€ search_results.csv



### API Keys    â”‚   â”œâ”€â”€ documents.jsonIn `main_enhanced.py`:



- **secrets.toml is NOT in the repository** (protected by .gitignore)    â”‚   â””â”€â”€ reddit_content.json

- `config.py` safely loads secrets at runtime using `tomli`

- Never commit `secrets.toml` to version control    â”œâ”€â”€ processed/                # Extracted statements```python

- Get your own API keys:

  - SerpAPI: https://serpapi.com (100 free searches/month)    â”‚   â””â”€â”€ statements.jsonTARGET_PAIRS = 1000  # Default: 500

  - Reddit: https://www.reddit.com/prefs/apps

    â””â”€â”€ final/                    # Annotated pairs```

### Data Privacy

        â””â”€â”€ pairs_for_annotation_*.csv

- All scraped content is from public sources

- Reddit scraping follows API terms of service```## ğŸ› ï¸ Troubleshooting

- Respects `robots.txt` and rate limits

- No personal data collection



------### Issue: Google Blocking Requests



## ğŸ“¦ Git LFS



Large data files (.csv, .json, .db) are tracked with Git LFS:## ğŸ”§ Configuration**Solution 1**: Use SerpAPI (recommended)



```bash- Sign up at https://serpapi.com (100 free searches/month)

# Already configured in .gitattributes

# Files are automatically managed by Git LFS### Key Parameters in `config.py`- Add API key to `config.py`



# To download data files after cloning

git lfs pull

``````python**Solution 2**: Increase delays



---# Number of queries to use (43 controversy-focused queries available)```python



## ğŸ› TroubleshootingAGRICULTURE_QUERIES = [...]  # 43 queries covering farm laws, MSP, subsidies, etc.SCRAPE_DELAY = 5  # Increase from 2



### Import Error: tomli```



```bash# Similarity threshold for pairing (0-1)

pip install tomli==2.0.1

```SIMILARITY_THRESHOLD = 0.3**Solution 3**: Use Selenium (slower but more reliable)



### SpaCy Model Not Found- Already in requirements



```bash# Maximum pairs from single URL combination- Modify scraper to use Selenium

python -m spacy download en_core_web_sm

```MAX_PAIRS_PER_SOURCE = 100



### Warning: secrets.toml not found### Issue: No Statements Extracted



This is normal if you haven't created `secrets.toml` yet. The pipeline will work without it using web scraping fallbacks, but:# Sentence Transformer model

- Google scraping will be slower and less reliable

- Reddit scraping will be disabledSENTENCE_TRANSFORMER_MODEL = "all-MiniLM-L12-v2"**Cause**: Scraping failed or content quality poor



Create `secrets.toml` with your API keys to enable full functionality.```



### Pipeline Hangs at Step 6**Solutions**:



**Expected behavior** - embedding computation requires significant GPU/memory. Options:### Customization- Check `data/raw/documents.json` to verify content

1. Let Step 5 complete, then stop (Ctrl+C)

2. Use a machine with better GPU/memory- Lower `MIN_STATEMENT_LENGTH` in config

3. Modify code to process in smaller batches

Edit `config.py` to:- Add more/better queries

### No Statements Extracted

- Add/modify agriculture queries (focus on controversies for better inconsistencies)- Target specific trusted domains

**Causes**:

- Scraping failed (check `data/raw/documents.json`)- Change target domains (government sites, news outlets, etc.)

- Content quality poor

- Filters too strict- Adjust similarity thresholds### Issue: No Pairs Generated



**Solutions**:- Configure scraping parameters

- Verify API keys in `secrets.toml`

- Check internet connection**Cause**: Statements too dissimilar

- Lower `MIN_STATEMENT_LENGTH` in `config.py`

- Add more/better queries---



### SerpAPI Quota Exceeded**Solutions**:



- Free tier: 100 searches/month## ğŸ“Š Pipeline Steps- Lower `SIMILARITY_THRESHOLD` in config (try 0.2)

- Wait for quota reset or upgrade plan

- Pipeline automatically falls back to web scraping- Collect more statements (use more queries/URLs)



---### Steps 1-5: Data Collection (Local Machine)- Focus queries on specific controversial topics



## ğŸ“ˆ Performance Stats (Real Run)



- **Queries Used**: 43 controversy-focused queries1. **SERP Scraping**: Query Google for agriculture-related content### Issue: Too Many Pairs

- **URLs Found**: ~3,400+

- **Documents Scraped**: 3,438 (success rate varies by source)2. **Content Extraction**: Scrape full text from URLs using multi-strategy extraction

- **Statements Extracted**: 22,198 total

- **Opinion Statements**: 4,413 (19.9%)3. **Reddit Scraping**: Collect posts and comments from agriculture subreddits**Solution**: Filter more aggressively

- **Average Statements/Document**: 6.5

- **Runtime**: ~60 minutes for Steps 1-54. **Statement Extraction**: Extract sentences with SpaCy, detect opinions, filter for relevance```python



---5. **Database Storage**: Save statements to SQLite databaseMAX_PAIRS_PER_SOURCE = 50  # Default: 100



## ğŸ¤ Contributing```



Contributions welcome! Please:**Output**: 

1. Fork the repository

2. Create a feature branch- ~3,400+ documents## ğŸ“Š Pipeline Parameters

3. Make your changes

4. Submit a pull request- ~22,000+ statements



---- ~4,400+ opinion statements (19.9%)Edit these in `main_enhanced.py`:



## ğŸ“„ License



MIT License - See LICENSE file for details### Steps 6-8: Pair Generation (Google Colab GPU)```python



---NUM_QUERIES = 10              # Number of queries to use



## ğŸ™ Acknowledgments6. **Embedding Computation**: Generate semantic embeddings using Sentence Transformers (GPU-accelerated)MAX_URLS_PER_QUERY = 10      # URLs to scrape per query



- **SerpAPI** for Google search API7. **Intelligent Pairing**: TARGET_PAIRS = 500            # Target pair count

- **Reddit API** (PRAW) for social media data

- **SpaCy** for NLP processing   - Compute cosine similarity matrixUSE_REDDIT = False            # Enable Reddit scraping

- **Sentence Transformers** for semantic embeddings

- **Hugging Face** for transformer models   - Quality scoring (bonus for same-source, opinions)```



---   - Stratified sampling (50% same-source opinions, 25% same-source mixed, etc.)



## ğŸ“§ Contact   - Diversity filtering (max 100 pairs per URL combo)## ğŸ“ Tips for Better Results



- GitHub Issues: [Report bugs or request features](https://github.com/XAheli/AgriIR_Query_Gen/issues)8. **Export**: Generate CSV with annotation columns

- GitHub: [@XAheli](https://github.com/XAheli)

### 1. Target Controversial Topics

---

**Output**: Focus queries on topics with multiple viewpoints:

## ğŸ—ºï¸ Future Enhancements

- 1,000+ diverse statement pairs- Farm laws debate

- [ ] Add support for Twitter/X scraping

- [ ] Implement automatic annotation suggestions- Ready for manual annotation- MSP policy changes

- [ ] Create fine-tuned inconsistency detection model

- [ ] Add multilingual support (Hindi, regional languages)- Subsidy programs

- [ ] Deploy as web service API

---- Agricultural reforms

---



**Star â­ this repo if you find it useful!**

## ğŸ“ Annotation Guide### 2. Mix Source Types

Combine:

### CSV Format- Government announcements (official stance)

- News articles (factual reporting)

The exported CSV contains:- Opinion pieces (subjective views)

- Social media (public opinions)

| Column | Description |

|--------|-------------|### 3. Same-Source Pairs

| `id` | Unique pair identifier |Prioritize pairs from same source/author:

| `statement_a` | First statement |- Better for detecting self-inconsistency

| `statement_b` | Second statement |- Shows evolution of positions

| `similarity_score` | Semantic similarity (0-1) |- More interesting contradictions

| `quality_score` | Quality score with bonuses |

| `same_source` | Both from same URL? |### 4. Opinion Statements

| `both_have_opinions` | Both contain opinions? |Look for statements with:

| `source_a` / `source_b` | Source URLs |- Modal verbs: should, must, need to

| `domain_a` / `domain_b` | Domain names |- Stance markers: support, oppose, believe

| `relationship_label` | **[TO ANNOTATE]** Unrelated/Consistent/Inconsistent |- Value judgments: better, worse, unfair

| `inconsistency_subtype` | **[TO ANNOTATE]** Surface/Factual/Value (if Inconsistent) |

| `notes` | Optional observations |## ğŸ” Data Quality Checks



### Annotation InstructionsBefore annotation, verify:



1. **Prioritize**: Start with pairs where `same_source=True` and `both_have_opinions=True`1. **Statement Quality**

2. **Label**: Fill `relationship_label` column:   - âœ… Complete sentences

   - `Unrelated`: Completely different topics   - âœ… Agriculture-related

   - `Consistent`: Can both be true   - âœ… Readable and clear

   - `Inconsistent`: Contradictory   - âŒ No promotional text

3. **Subtype**: If `Inconsistent`, fill `inconsistency_subtype`:   - âŒ No navigation text

   - `Surface`: Direct logical contradiction

   - `Factual`: Conflicting facts/numbers2. **Pair Quality**

   - `Value`: Conflicting opinions/values   - âœ… Semantically similar (overlap in topic)

4. **Notes**: Add any observations to help with model training   - âœ… Both are substantial statements

   - âœ… Worth comparing for consistency

**Target**: Annotate 300+ pairs for robust dataset.   - âŒ Not identical/near-duplicate

   - âŒ Not completely unrelated

---

## ğŸ“ˆ Expected Output

## ğŸ”’ Security & Privacy

For a typical run with 10 queries:

### API Keys- **URLs found**: 50-100

- **Documents scraped**: 30-70 (60-70% success rate)

- **Never commit** `secrets.toml` to GitHub- **Statements extracted**: 500-2000

- Use `secrets.toml.example` as template- **Candidate pairs**: 1000-5000

- API keys are loaded at runtime from `secrets.toml`- **Final pairs**: 500 (after filtering)

- `.gitignore` configured to exclude secrets- **For annotation**: Select best 200-300



### Data Privacy## ğŸ”— Useful Resources



- All scraped content is public data### APIs & Tools

- Reddit scraping follows API terms of service- SerpAPI: https://serpapi.com (Google scraping)

- Respects `robots.txt` and rate limits- Reddit API: https://www.reddit.com/prefs/apps

- SpaCy: https://spacy.io/usage/models

---

### Open Source Scrapers

## ğŸ“¦ Git LFS Setup- newspaper3k: Article extraction

- trafilatura: Web content extraction

Large data files (.csv, .json, .db) are tracked with Git LFS:- PRAW: Reddit API wrapper

- snscrape: Twitter scraping

```bash

# Install Git LFS### Agriculture Sources

git lfs install- PIB: https://pib.gov.in

- Ministry of Agriculture: https://agricoop.nic.in

# Track data files (already configured in .gitattributes)- Down to Earth: https://downtoearth.org.in

git lfs track "*.csv"- The Hindu (Agriculture section)

git lfs track "*.json"- Indian Express (Rural section)

git lfs track "*.db"



# Add and commit
git add .gitattributes
git add data/
git commit -m "Add data files with LFS"
git push
```

---

## ğŸ› Troubleshooting

### Import Error: tomli

```bash
pip install tomli==2.0.1
```

### SpaCy Model Not Found

```bash
python -m spacy download en_core_web_sm
```

### GPU Out of Memory (Local Machine)

Use Google Colab for Steps 6-8:
1. Stop local pipeline after Step 5
2. Upload `statements.json` to Colab
3. Run `colab_pair_generation.ipynb`

### SerpAPI Quota Exceeded

- Free tier: 100 searches/month
- Upgrade or wait for quota reset
- Pipeline falls back to web scraping (slower)

### Reddit API Errors

- Check credentials in `secrets.toml`
- Verify Reddit app permissions
- Ensure user agent is correctly formatted

---

## ğŸ“ˆ Performance & Scalability

### Current Stats (Real Run)

- **Queries**: 43 controversy-focused queries
- **URLs Scraped**: 3,438 documents
- **Statements Extracted**: 22,198 (4,413 with opinions)
- **Pairs Generated**: 1,000+ diverse pairs
- **Time**: ~90 minutes total (60 min local + 10 min Colab + 20 min annotation)

### Scaling Up

To increase dataset size:
1. Add more queries to `config.py`
2. Increase `MAX_URLS_PER_QUERY` in `main_enhanced.py`
3. Adjust `TARGET_PAIRS` for more pair generation
4. Use more powerful GPU for faster embeddings

---

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ™ Acknowledgments

- **SerpAPI** for Google search API
- **Reddit API** (PRAW) for social media data
- **SpaCy** for NLP processing
- **Sentence Transformers** for semantic embeddings
- **Hugging Face** for transformer models

---

## ğŸ“§ Contact

For questions or issues:
- Open a GitHub issue
- Email: [Your email]
- GitHub: [@XAheli](https://github.com/XAheli)

---

## ğŸ—ºï¸ Roadmap

- [ ] Add support for Twitter/X scraping
- [ ] Implement automatic annotation suggestions
- [ ] Add data augmentation techniques
- [ ] Create fine-tuned inconsistency detection model
- [ ] Deploy as web service API
- [ ] Add multilingual support (Hindi, other Indian languages)

---

**Star â­ this repo if you find it useful!**
